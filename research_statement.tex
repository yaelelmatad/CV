\documentclass[11pt]{article}
\usepackage[left=1.5cm,top=1.5cm,right=1.5cm,bottom=1.5cm]{geometry}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{ulem}
\usepackage[draft]{pdfpages}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\def \dbar{{\mathchar'26\mkern-12mu d}}
\newcommand{\kB}{k_{\mathrm{B}}}
\newcommand{\traj}{\text{traj}}
\newcommand{\ee}{\mathrm{e}}
\newcommand{\tobs}{t_\mathrm{obs}}
\newcommand{\eps}{\epsilon}
\newcommand{\sig}{\sigma}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\WW}{\mathbb{W}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathcal{F}}

\newcommand{\rate}{\lambda}
\newcommand{\upr}{\gamma}

\newcommand{\zz}{\ee^{-s}}
\newcommand{\zzs}{\ee^{-s^*}}

\newcommand{\Sc}{S}

\renewcommand{\figurename}{\textbf{Figure}}

% Different font in captions
\newcommand{\captionfonts}{\small}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter

\renewcommand{\abstractname}{Summary}

 \def\dsp{\def\baselinestretch{2.0}\large\normalsize}
%\dsp

\title{{{\textbf{Statement of Research Interests}}: \\Out of equilibrium dynamical systems, the path from \textit{A} to \textit{B}.}}
\author{Yael S. Elmatad}
\date{}                                           % Activate to display a given date or no date
\begin{document}
\maketitle
%\section{}
%\subsection{}
%\includepdf[pages=-]{pdfpages.pdf}
%\section{Long-Term Objectives}
\begin{abstract}
Increasingly, statistical physics has had to grapple with scientific questions in which techniques of classical, equilibrium thermodynamics have to be expanded in order to apply to inherently slow and often driven systems.   As these systems are out-of-equilibrium, it does not suffice to simply study ensembles of configurations. Instead, we are forced to examine dynamical ensembles of trajectories.  One such problem is the glass transition.  Reconciling the apparent disparity between the microscopic, disordered nature of glass and its macroscopic rigidity has confounded physicists for years. However, we have recently shown how studying a simple model in trajectory space can shed light on the true nature and universality of molecular glassformers.  While these simple models have given us new insights into glasses, they are not the complete picture. I am particularly interested in investigating the effects of probe particles on supercooled liquids.  Specifically, I seek to understand to what extent probe particles are recorders of the dynamics of the host.  Using techniques developed to study dynamical systems (eg. transition path sampling), which I have already applied to simple models, I believe it would be instructive to study other out of equilibrium processes such as motion in a crowded intracellular environment and clot formation in sickle-cell anemia.  Beyond these applications, I am intrigued by the development of new dynamical sampling methods.  One such example would be investigating new ways to perform parallel tempering in trajectory space which could help improve sampling efficiency in path ensembles.  Moreover, I am fascinated by protocols for designing novel materials by optimizing model parameters to create an ideal process for observing a desired effect (say, self assembly).  In effect, such a method would shift the focus from studying a set of trajectories given particular parameters, to studying a model with adjustable nobs which could be tuned to obtain a desired effect. Inverting the question would allow us to not only learn more about the physical world we live in but also to gain new ways to engineer models and novel materials to perform desired behaviors.


%Increasingly, statistical physics has had to grapple with a new set of scientific questions where techniques of classical, equilibrium thermodynamics have to be stretched to inherently slow, out-of-equilibrium, and sometimes driven systems.  Since these systems are out-of-equilibrium it does not suffice to simply study ensembles of configurations, instead, we are forced to examine dynamical ensembles of trajectories.  One such problem is the problem of the glass transition.  Reconciling the apparent disparity between the microscopic, disordered nature of glass and its macroscopic rigidity has confounded physicists for years. However, we have recently shown how studying a simple model in trajectory space can shed light on the true nature and universality of molecular glassformers. In this statement I propose to further this work not only to continue my study of glasses but also to extend the methods developed to study these slow systems to other systems such as the biophysical problem of crowded cellular environments.  Beyond applications, I propose to extend the dynamical sampling methods even further, thus allowing us to not only learn more information about the physical world we live in but also gain new ways to engineer models and novel materials to preform desired behaviors.

%Glassy materials have wide ranges of applications, including nuclear storage~\cite{Sales1984}, cryobiology~\cite{Lemler2004}, and pharmaceuticals~\cite{Craig1999}. Understanding glassy phenomena would lead to advancements in these fields, such as extending the life of organs for transplant~\cite{Lemler2004} and improving the safe long-term storage of nuclear waste~\cite{Sales1984}.  However, a strong impediment to the effective design of glassy materials with desired properties is the lack of a detailed understanding of the very nature of the glass transition, and whether it is thermodynamic or kinetic in origin. Our earlier work, utilizing an advanced trajectory sampling technique called transition path sampling (TPS)~\cite{Bolhuis_AnnuRevPhysChem_2002}, has taught us a great deal about the origin of the glass transition in model glassformers. However, new techniques are still needed to improve the sampling of realistic glass models as well as to design new materials that are glassy~\cite{Whitesides2002}. Current methods involve scanning parameter space by trial and error and conducting long, time-consuming simulations of material formation for each trial parameter set. Here, I propose a method which optimizes parameters for a given model based on path probabilities for a transition path ensemble.  This method circumvents these problems by making forays in thermodynamic parameter space and extends the technique of parallel tempering to allow for sampling in both trajectory and parameter space.  These techniques will also facilitate the engineering of synthetic self-assembling nanomaterials, which have applications to drug delivery~\cite{Rosler2001} and new materials~\cite{Whitesides2002}.   Moreover, I believe these methods have wide applicability to related sampling and optimization problems.
\end{abstract}

%\section*{Aims}
\section{The Glass Transition}
Supercooled liquids and the glass transition are a much debated, poorly understood physical phenomenon.  The nature of supercooled liquids is inherently out of equilibrium as the equilibrium state (crystal) is avoided by a cooling protocol that frustrates the previously in-equilibrium liquid.  In recent work, we have shown how to take a wide range of experimental data for molecular glassformer transport properties and collapse them onto a single curve. Much of the data taken on supercooled liquids originates from measurements that involve probe particles -- guest particles that are used as `thermometers' for their host material, acting as recorders of transport properties.  To what extent these probes can record useful information about the material in which they are embedded is unknown.  Probe particles are generally designed to be as ``inconspicuous" as possible to best mimic the behavior of the host.  However, this passivity is often taken {\it a priori} and unverified. I believe this unverified claim to be of utmost importance to our understanding of glassy measurements. For example, would swelling the probe size tell us something about inherent length scales?   Once the probe size is of the order of the typical distance between dynamic heterogeneities (a hallmark of `glassy' dynamics), would the mean field behavior of an average liquid be recovered? In this regime, would the diffusion constant and the relaxation time  be coupled as per the Stokes-Einstein equation, even while the host supercooled liquid exhibits decoupling? I believe that Monte-Carlo and molecular dynamics simulations would be excellent tools for studying the effects of probe particles on host liquids.  Through these studies, it would be understood to what extent the probe particle can be used as a molecular thermometer for the host and, moreover, to what extent and in what regime does the probe disturb the normal dynamics of the host liquid.  These insights would be useful to experimentalists who often assume that the probe acts only as an ideal `passive' recorder.  
As these ideas are a logical next step of work I have already begun pursuing, it would make an ideal project for a beginning graduate student seeking to learn glassy phenomenology and simulation techniques.  

\section{Crowded Environments}
Recently, many have touted the analogy between crowded cellular environments and the colloidal glass transition.  By changing the osmotic pressure, experimenters were able to `jam' the interior environment, just as they would a dense colloidal or granular material.  While this pressure was applied in a laboratory, this change in pressure is similar to the pressure change during tumor formation as cells become more and more densely packed due to rapidly dividing cells.  Since cells are crowded, out of equilibrium environments they share many features with supercooled liquids -- also dense packings of molecules -- only on a different scale.  Recent numerical and simulation technique advances have been used to shed light on motion in such supercooled environments.  By exploiting the similarity, large molecular simulations of a dense cellular system coupled with new techniques can be used to study basic motion of small proteins as they diffuse in such a crowded environment.  These simulations could use techniques of transition path sampling, which I am familiar with based on my studies of glassformers, to characterize the ensemble of possible moves as a protein escapes its environment by exiting its macromolecular cage.  I would be especially interested in discussing these topics with biologists and biophysicists.  I believe that their biological expertise coupled with my familiarity with work in dense, glassy systems would help shed new insights into crowded cellular environments.  Moreover, I believe that through collaboration we would be better poised to answer questions about motion within a cell.  We can also compare these motions to the facilitated motions in granular and colloidal materials.  This information would lead to a better physical understanding of such a complex, crowded environment.   

Beyond intracellular material, analogies with jammed materials are also observed when examining {\textit{ extra}}-cellular materials. Recent work in microfluidic devices have shown how sickle-cell anemia manifests itself when cells change their shape and form clots is narrow veins.  The geometry of the cells as they ``sickle" has been shown to play an important role in the disease.  Understanding elementary processes about how clots form and dissolve can be of use to create new treatments for the disease.  It has already been shown that a (toxic) chemical inhibitor that binds to red blood cells can alleviate clot formation.  %Unfortunately, this inhibitor is toxic.  
Alternatively, another treatment might be to add an intruder that frustrates the formation of such clots geometrically, rather than chemically.  The elementary steps of formation and destruction of such clots can be studied using techniques of transition path sampling.  These simulations would add insights about this physical phenomenon that has devastating medical consequences.  Moreover, this knowledge might inform what size and shape molecules would be best for treatment, perhaps offering a non-toxic alternative to chemical binding.  Again, here I would seek collaborations with biologists to combine my physical understanding of glasses with their biological knowledge.    
Such a project would be well suited for a more advanced graduate student as it requires a deeper understanding of simulation techniques and the ability to assimilate new, biological information. Plus, it would allow for the possibility of collaborative, interdisciplinary work ideal for students near the end of their degree.

\section{Sampling Methods}

%In a recent interview, when asked what the greatest challenges for computational materials science is, pioneering researcher Michele Parrinello stated that ``...sampling is really the most important aspect to be improved...''.  
The problem of efficient and accurate sampling methods for complex systems is one that continues to challenge researchers even in the face of great technological and algorithmic gains.  To that end, in the future it will be necessary to expand methods to improve sampling and design in materials.  For example, the development of a new parameter space sampling technique to investigate ensembles of trajectories with increased computational efficiency for dynamical problems will be useful.  On the other hand, scientists and engineers are seeking methods which would optimize parameters to select for certain dynamical behaviors.  The creation of such methods would allow us to engineer models which strike a balance between thermodynamics and kinetics.  Both methods would have applications for studying glassformers, biophysics, and designing novel materials.

%\section*{Background and Significance}

%\subsection*{Glassy behavior is a ubiquitous phenomenon with wide applicability}
%Glasses are formed when liquids are supercooled beyond their freezing temperature rapidly enough to avoid crystal formation, forming a ``metastable" but long lived state that preserves the liquid structure \cite{Ediger1996, Angell_Science_1995}.   Understanding the apparent disconnect between the macroscopic solid-like properties of a glass and the microscopic liquid-like properties has, however, eluded glass physicists for some time. Such an understanding of glassy phenomena is essential because it has applications to not only material science \cite{Schulli2010} but also has been recently implicated in a wide variety of processes such as jamming in granular material \cite{Clusel2009}, biological cells under compression \cite{Zhou2009}, and protein folding \cite{Gin2009}. Glasses can be used as a means for nuclear waste storage \cite{Sales1984} as well as for making pharmaceuticals more shelf-stable \cite{Craig1999}. One of the most polarizing debates surrounding glasses has concerned the importance of dynamics compared to thermodynamics in glassformers.  There are many who purport that glassy behavior is entirely thermodynamic \cite{Lubchenko_AnnuRevPhysChem_May_2007}. Others, however, have contended that it is a purely dynamic phenomenon \cite{Garrahan2003}.

%\subsection*{Kinetically constrained models capture glassy dynamic heterogeneity}
%To investigate the dynamics of glassformers, my colleagues and I have been studying kinetically constrained models (KCMs) such as the Fredrickson-Andersen model (FA) \cite{Fredrickson_PhysRevLet_Sept_1984} .  This class of models is motivated by experiments of glassformers showing that glassy systems experience dynamic heterogeneity: that some regions of glassformers relax faster than others \cite{Ediger1996, Angell_Science_1995}.  These dynamic heterogeneities have been linked to a phenomenon known as facilitation. An excitation is thought by many to be the mechanism by which supercooled liquids relax. As an excitation travels through the system it brings local mobility to the region.  A region is more likely to become mobile itself if a neighboring region has been recently active, thus mobility is facilitated by neighboring sites \cite{Keys_NatPhys_Apr_2007}.  Moreover, we have shown glass universality in experimental systems using a theory derived from KCMs by showing that relaxation times for over 50 glassformers collapse to a single curve, thus supporting the idea that kinetic effects dominate glassy behavior \cite{Elmatad2009}. KCMs are models whose underlying thermodynamics are very simple, but whose glassy behavior is due to a purely kinetic constraint.  In the case of the FA model it is just that of a 1 dimensional lattice gas (non-interacting spins on a lattice) \cite{IMSM}. Facilitation is added through a kinetic constraint.

%\subsection*{Ensembles of trajectories using Transition showcase the nature of Kinetically Constrained Models}
%Using simple models for glassformers known as kinetically constrained models (KCMs), we previously studied the dynamical ensemble (i.e., the ensemble of trajectories) rather than a static ensemble (i.e., the ensemble of configurations).  We found that this model undergoes a dynamic first-order phase transition. That is, there is a discontinuity in an order parameter, $K$, which measures the ``activity'' in the model (in our case, the number of configuration changes) which is coupled to a dynamical field $s$.  This behavior was qualitatively demonstrated for atomistic models \cite{Hedges_Science_2009} as well.  Upon mild constraint softening, the first-order dynamical phase transition does persist, though eventually the system goes through a critical point and then finally into a one-phase region which approaches a lattice gas \cite{Elmatad_PNAS_2010}.  This dynamical phase transition is highlighted in Figure \ref{fig:softFA} alongside a phase diagram in the $s$ and $\epsilon$ plane. $\epsilon$ is the measure of ``softness'': $\epsilon=0$ is the pure FA model and $\epsilon \rightarrow \infty$ recovers the lattice gas. 

%\begin{figure}[b!] %  figure placement: here, top, bottom, or page
%   \centering
 %  \includegraphics[width=5in]{softFA.pdf} 
 %  \caption{Example of a dynamical phase transition in the soft FA model. $\langle k \rangle_{s}$ is the intensive equivalent of $K$, where $k$ is the activity per space-time unit volume.  (a) shows the phase diagram in the $\epsilon$, $s$ plane.  Inset shows the probability distribution at a value of $s$ indicated by the dots in the phase diagram. (b) shows $\langle k \rangle_{s}$ as a function of $s$ at the same state points as the inset of (a) and the dots indicated in (a).  Labeling and colors are the same throughout.  For more, see \cite{Elmatad_PNAS_2010}.}
%   \label{fig:softFA}
%\end{figure}

To investigate behaviors of rare events, we have used advanced sampling methods developed to study ensembles of trajectories. One of the most significant of these is transition path sampling. %\cite{Bolhuis_AnnuRevPhysChem_2002}.  
This method has allowed people to probe a diverse set of dynamical problems from supercooled liquid dynamics %\cite{Merolle_PNAS_Aug_2005} 
to biological self-assembly. %\cite{TenWolde2002}. 
While current methods can investigate trajectory space for fixed parameters very well, few techniques exist which allow for crossing large barriers in parameter space. Traversing dynamical barriers in trajectory space suffers the same pitfalls as traversing barriers in configuration space: barriers in space-time can prove too high to climb in a given parameter set, meaning that important regions of trajectory space may go unexplored.  If this is the case, the transition path ensemble will be poorly converged and may not reflect the true nature of the underlying dynamical free energy landscape. In fact, even with our simple models, straightforward sampling leads to a large number of rejected trajectories.

%\subsection*{Parallel tempering is a useful tool for overcoming barriers in parameter space}

Parameter space methods for parallel tempering %\cite{Swendsen1986} 
have been mostly limited to swapping static configurations with different parameters by taking into account the relative probabilities of these states.  However, recent works have shown that it is possible to extend these methods to include other parameters. I believe it will be possible to extend these methods to mix both dynamical order parameters (which characterize a trajectory) as well as thermodynamic parameters (such as the temperature).  In order to make such a trajectory space parallel tempering method fully functional, the path probability for the entire trajectory must be calculated - requiring storing every step of the simulation.  This computational demand has only recently become tractable with advances in computing -- both in hardware as well as architecture designed to take advantage of the parallel nature of supercomputers.   Using such a technique, the path probability could be reconstituted for any arbitrary set of parameters and thus parallel tempering-like swaps could be attempted between two trajectories run with different thermodynamic and dynamical parameters.  This work could potentially be expanded to swapping between ``protocols" (eg. cooling a liquid toward its glass transition). Take, for example, a trajectory whose temperature changes as a function of time. As long as the path probability of that trajectory can be computed for any arbitrary protocol, swaps can be attempted between these protocols.  Such methods would be easily verified on lattice models such as kinetically constrained glassy models (which I have studied extensively) and lattice protein models -- both known to exhibit rich dynamic phenomena.  

%Parallel tempering allows configurations to be swapped among different parameters (say, varying temperatures, $T$) using the conjugate variable (for $T$ that is energy, $E$).  This calculation allows you to compute the likelihood of finding a configuration generated using one parameter set $\{\lambda_{i}\} = \{\lambda_{1} \lambda_{2}, \lambda_{3},...,\lambda_{n}\}$ in a separate parameter space $\{\lambda'_{i}\} = \{\lambda'_{1} \lambda'_{2}, \lambda'_{3},...,\lambda'_{n}\}$.  By using these methods, scientists have been able to computationally explore regions of phase space which would have otherwise been nearly impossible to probe.  The usual set up requires calculating the Boltzmann factor $P \propto e^{-\beta E(\vec{x})}$ where $\beta = 1/\kB T$, $\kB$ is Boltzmann's constant, $T$ is temperature, and $E(\vec{x})$ is the energy associated with a specific configuration, $\vec{x}$.  



%\subsection*{A dynamical extension of parallel tempering is needed in trajectory space}

%The simplest way to extend this method to trajectories is to make the analogy to a dynamical order parameter, such as the activity $K$  (whose configurational equivalent is energy) and then couple it to a conjugate field $s$ \cite{Garrahan_JPhysA_2008}. This treats trajectories as if they are ``configurations" in $d+1$ dimensions.  By running trajectories at different values of $s$ and swapping between them with a new ``dynamical" Boltzmann factor $P_\mathrm{dyn} \propto e^{-s K}$ one recovers a similar result to configuration space parallel tempering. While this method works well for getting over barriers in the space of dynamical parameters, it does not actually alter the dynamics within a trajectory to bias it towards the correct ensemble.  That is, it does not change simulation parameters which control kinetics such as $T$ or interaction strength. Others have proposed methods for trajectory space parallel tempering but have limited it to deterministic dynamics and temperature, rather than generalizing to stochastic dynamics and any generic simulation parameter \cite{Vlugt2001}.

\section{Material Design}

In the last twenty years, nanoparticles and their applications have come to the forefront of scientific consciousness.
%\cite{Glotzer2007}.  
Certain nanoparticles can be used for self-assembly, spontaneously forming complex aggregated structures. The future will hold a variety of uses for particles that can self-assemble such as for targeted drug delivery,
%\cite{Rosler2001}, 
in optics,
%\cite{Jiang2007}, 
and as surfactants.
%\cite{Glotzer2007}.  
The final, assembled, structure depends on the interactions between the nanoparticles and how those interactions dictate kinetic pathways to assembly. It has been noted that interactions should be strong but {\it not too strong}, so that an unassembled system has enough time and energy to bind but also to anneal out defects.
%\cite{Hagan2006, Whitelam2008}.  
Many have tried to design methods to optimize specified assembly pathways (and final structures) at viable conditions.  However, a reliable method remains elusive. 

%\begin{figure}[b!] %  figure placement: here, top, bottom, or page
 %  \centering
%   \includegraphics[width=5in]{flowchart.pdf} 
%   \caption{Flow diagram for proposed dynamic tempering algorithm.  Blue boxes indicate steps which are significantly different from the usual TPS scheme with dynamic order parameter, $K$, and field, $s$. Shooting and shifting are the elementary moves of perturbing trajectories in TPS \cite{Bolhuis_AnnuRevPhysChem_2002}.}
%   \label{fig:flow}
%\end{figure}




%\section*{First Stage: Probing Parameter Space to Learn About Trajectories}

Tuning interactions until they produce a desirable effect involves developing a method to allow trajectories to explore parameter space. For specific parameters of interest, one can gain insight into dynamics by sampling this space.  This allows us to sample the extreme values of our ensemble distribution along with the average.   %\subsection*{Algorithm}
%\textbf{1}. Choose a parameter set of interest for simulation, $\{\lambda_{1}, \lambda_{2},...,\lambda_{n}\}$.  Here $\lambda_{i}$ represents some control parameter (for example temperature, $T$) of which there are $n$ such parameters.
%\textbf{2}. Equilibrate trajectories for this system using a path sampling method such as TPS, forward flux sampling \cite{Allen2009} or the string method \cite{E2002}. This can include applying a dynamical field $s$ that couples to some extensive, dynamical variable, $K$.
%\textbf{3}. Choose a new set of parameters $\{\lambda_{1}^{\prime}, \lambda_{2}^{\prime},...,\lambda_{n}^{\prime}\}$.
%\textbf{4}. Simulate the trajectory using the dynamics associated with $\{\lambda_{1}^{\prime}, \lambda_{2}^{\prime},...,\lambda_{n}^{\prime}\}$.
%\textbf{5}. Accept-or-reject these trajectories based on the criterium:
%\begin{align*}
%P_{\mathrm{acc}}&=\min\left \{1,e^{-s \Delta K} \frac{P(\vec{X}^{\prime}|\{\lambda_{1}, \lambda_{2},...,\lambda_{n}\})}{P(\vec{X}|\{\lambda_{1}, \lambda_{2},...,\lambda_{n}\})} \cdot  \frac{P(\vec{X}|\{\lambda_{1}^{\prime}, \lambda_{2}^{\prime},...,\lambda_{n}^{\prime}\})}{P(\vec{X}^{\prime}|\{\lambda_{1}^{\prime}, \lambda_{2}^{\prime},...,\lambda_{n}^{\prime}\})} \right \}
%\end{align*}
%Here, the first factor is the dynamical order parameter equivalent to accepting trajectories based on changes in the dynamical variable $K$ at fixed $\{\lambda_i\}$. This fraction represents the relative weight of the two trajectories in the biased ensemble with field strength $s$.   $P(\vec{X}|\{\lambda_{1}, \lambda_{2},...,\lambda_{n}\})$ is the probability of observing a trajectory $\vec{X}$, a collection  of configurations at various time points $\vec{X}=\{\vec{x},\vec{x}_{1},...,\vec{x}_{N}\}$ given a certain set of parameters $\{\lambda_{1}, \lambda_{2},...,\lambda_{n}\}$. %Here we note that the fraction of the ratios of the initial trajectory ($\vec{X}$) and the new trial trajectory ($\vec{X}^{\prime}$) is the same as the exponential of the negative of the change in action, $\mathcal{E}$, of the trajectory:
%$$ \Delta \mathcal{E}(\vec{X} \rightarrow \vec{X}^{\prime}|\{\lambda_{1}, \lambda_{2},...,\lambda_f{n}\}) =  - \ln \left [  \frac{P(\vec{X}^{\prime}|\{\lambda_{1}, \lambda_{2},...,\lambda_{n}\})}{P(\vec{X}|\{\lambda_{1}, \lambda_{2},..\lambda_{n}\})}  \right]$$
%\textbf{6}. Repeat process from step 3. A flow diagram for this process is given in Figure \ref{fig:flow}.
%\subsection*{Test Cases for the First Stage: KCMs}
%To test the effectiveness method,  I will use kinetically constrained models of glassformers.  Specifically, I will test the method using the East \cite{Ritort_AdvInPhys_2003} and the FA model using a trajectory biasing field, $s$.  While the 1d FA model has been well characterized \cite{Garrahan_JPhysA_2008, Elmatad_PNAS_2010}, the hierarchical East model has not been studied in the context of a dynamical phase transition and studying the nature of this expected transition would add new knowledge to the field.  Since the theory \cite{Garrahan2003} which has successfully predicted glass universality~\cite{Elmatad2009} is based on hierarchical modelsthis phase transition is of particular interest. 
%\section*{Second Stage: Using Trajectory Space to Optimize Parameters}
%\begin{figure}[b!] %  figure placement: here, top, bottom, or page
 %  \centering
  % \includegraphics[width=5in]{flowchart_part2.pdf} 
  % \caption{Flow diagram for second phase.  Blue boxes indicate departures from usual transition ensemble harvesting methods.}
   %\label{fig:flow2}
%\end{figure}
This information can be used to optimize parameters that select for a particular behavior in a system with two basins, say $A$ and $B$.  For example, in a self-assembly process $A$ could be an unassembled state and $B$ an assembled state.  Ideally, we could optimize the probability of seeing a trajectory that goes from $A$ to $B$ given that the trajectory starts in $A$; that is, optimize, relative to the thermodynamic parameters, the probability of ending in state $B$ given that the system started in state $A$.

%\subsection*{Algorithm}
%\textbf{1}. Generate an ensemble of trajectories that go from $A$ to $B$ in an initial parameter space $\{\lambda_{1}, \lambda_{2}, .. \lambda_{n}\}$.  An example could be the interaction strength between particles.  For each trajectory $\vec{X}$ the probability is given by:
%$$P (\mathrm{\vec{X}}|\{\lambda_{i}\})= P_{0}(\vec{x}_{0}|\{\lambda_{i}\}) \prod_{t=0}^{t_{\mathrm{obs}}} p\left(\vec{x}(t |\{\lambda_{i}\}) \rightarrow \vec{x}(t+\Delta t | \{\lambda_{i}\})\right) $$
%Where $P_{0}(\vec{x}_{0}|\{\lambda_{i}\})$ is the probability of the initial configuration $\vec{x}_0$ and involves estimating the partition function of the system.  $p\left(\vec{x}(t| \{\lambda_{i}\}) \rightarrow \vec{x}(t+\Delta t|\{\lambda_{i}\})\right)$ is the probability of taking a step from configuration $\vec{x}(t| \{\lambda_{i}\})$ at time $t$ to configuration $\vec{x}(t+\Delta t| \{\lambda_{i}\})$ at time $t+\Delta t$ which is parameterized by the current parameter set $\{\lambda_{i}\}$.
%\textbf{2}. Given a set of trajectories, calculate the values of $\{\lambda_{i}\}$ for which these trajectories are most probable. First compute\footnote{In principle, one would really try to maximize the logarithm of $P (\mathrm{\vec{X}}|\{\lambda_{i}\}) $ for computational convenience.  Moreover, one would do this over all values of $\lambda$ as well over the entire ensemble of trajectories.}, $\partial P (\mathrm{\vec{X}}|\{\lambda_{i}\})/\partial \lambda_{i}$.
%For systems on a lattice with simple thermodynamics or systems with Langevin dynamics \cite{IMSM} whose initial conditions do not depend on $\{\lambda_{i}\}$ (say, because particles are too far apart to interact)\footnote{This avoids an expensive calculation of the equilibrium partition function contained in $P_{0}$.}, expressions for these derivatives can be analytically derived in a straightforward manner.    
Optimization methods for these kinds of problems have been recently promoted and are known as {\it maximum likelihood estimators}. %\cite{Crooks2007, Shirts2008, Minh2009}  
These methods can be then used iteratively until an ideal set of parameters is found that increases the probability of going from $A$ to $B$.  This method would act as a maximization method for path probability in parameter space.  These results can be informed by real thermodynamic control parameters that experimentalists can tune -- such as the interaction strength between particles (say, by changing the host material) or by tuning an external field.\\
%.  Use one of these methods to solve for: $\partial P (\mathrm{\vec{X}}|\{\lambda_{i}\})/\partial \lambda_{i} = 0$.
%\textbf{3}. Adopt these new parameters, $\{\lambda_{i}'\}$, and return to step 1.
%\textbf{4}. Repeat process until the parameters $\{\lambda_i\}$ have converged within a given tolerance.  A flow diagram for this process is given in Figure \ref{fig:flow2}.
%\subsection*{Test Cases for Second Stage: Self-Assembly}
%To test this method, I will investigate self-assembly of Janus particles using Brownian dynamics \cite{Jiang2007}.  Janus particles have two opposing faces. For example, one side can be hydrophobic and the other hydrophilic.  These particles can have varied shapes, interaction strengths, and relative compositions and may be useful for technology such as optics \cite{Glotzer2007}.  Tuning these parameters to give specified structures (for example, strings, bilayers, or vesicles) would be a valuable way to determine how parameters play into dynamical pathways. 

%\section*{Long Term Goals}
%These methods would help us understand the link between parameter and trajectory space.  For glassformers, I plan to investigate the links between biased trajectory ensembles and driven systems \cite{Speck2010}. Such connections are tantalizing because they are essential to the idea of facilitation. This would suggest that a link between a biasing field, $s$, in trajectory space and an experimentally measurable system can be found.  In the long term, I hope to use these methods to study the properties of dynamic facilitation since the methods will make simulating complex models more computationally feasible. My other goal is to  design novel materials by understanding self-assembly optimization - such as in the formation of Janus particles into various ordered structures \cite{Glotzer2007}.   The potential to design materials which preform a specified function but whose optimal parameters are not known {\it a priori} is one of the holy grails of theoretical research.  I hope these two methods - and the understanding which comes from employing them - will help make significant advancements in these fields, allowing me to add an increased understanding of the interplay between dynamics and thermodynamics to the scientific community.
%\bibliography{library}

\noindent These sampling ideas could form the basis for a graduate thesis or, equally for a postdoctoral project.  Moreover, these ideas would be especially well suited for collaborations with material scientists.  
%\bibliographystyle{Science}

\end{document}  
